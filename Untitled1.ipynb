{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 200000 rows.\n",
      "(200000, 37)\n",
      "   duration       sip       dip  source_port  dest_port  forward_status  \\\n",
      "0    48.380 -0.154065 -0.629890           53         53               0   \n",
      "1    48.380 -0.605569 -0.189528           53         53               0   \n",
      "2    50.632  0.421506  0.850835           80       1838               0   \n",
      "\n",
      "   type_service  pack_exch  bytes  attack_tag  ...  flags-.A.RS.  \\\n",
      "0             0          2    209           0  ...             0   \n",
      "1             0          2    167           0  ...             0   \n",
      "2             0          9   2082           0  ...             0   \n",
      "\n",
      "   flags-.A.RSF  flags-.AP...  flags-.AP..F  flags-.AP.S.  flags-.AP.SF  \\\n",
      "0             0             0             0             0             0   \n",
      "1             0             0             0             0             0   \n",
      "2             0             1             0             0             0   \n",
      "\n",
      "   flags-.APR..  flags-.APR.F  flags-.APRS.  flags-.APRSF  \n",
      "0             0             0             0             0  \n",
      "1             0             0             0             0  \n",
      "2             0             0             0             0  \n",
      "\n",
      "[3 rows x 37 columns]\n",
      "Train on 150000 samples, validate on 50000 samples\n",
      "Epoch 1/500\n",
      " - 6s - loss: 0.0523 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 2/500\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 3/500\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 4/500\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 5/500\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 6/500\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 7/500\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 8/500\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 9/500\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 10/500\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 11/500\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 12/500\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 13/500\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 14/500\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 15/500\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 16/500\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 17/500\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 18/500\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 19/500\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 20/500\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 21/500\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 22/500\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 23/500\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 24/500\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ae16b41edf95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m# monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;31m# model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=1000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/tf_jupyter/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/miniconda3/envs/tf_jupyter/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    190\u001b[0m                                     \u001b[0;34m'If using HDF5 input data, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                                     'pass shuffle=\"batch\".')\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow.contrib.learn as skflow\n",
    "import string\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#####################################################\n",
    "#os.system('python 22_05_Transform_columns.py')\n",
    "#No puedo ponerlo asi por que entonces dice que no esta definido df\n",
    "#Seguro que hay alguna manera\n",
    "#####################################################\n",
    "\n",
    "path = \"../july_reduced copia.csv\"\n",
    "# This file is a CSV, just no CSV extension or headers\n",
    "df_not_chunk = pd.read_csv(path, header=None, chunksize=200000)\n",
    "\n",
    "##########################CHUNKSIZE###################################################\n",
    "chunk_list = [] #append echa chunk df here\n",
    "\n",
    "#Each chunk is in a df format\n",
    "for df in df_not_chunk:\n",
    "    #si no imprime no funciona, da error en el print\n",
    "    #print(df)\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "    print(\"Read {} rows.\".format(len(df)))\n",
    "    # df = df.sample(frac=0.1, replace=False) # Uncomment this line to sample only 10% of the dataset\n",
    "    df.dropna(inplace=True,axis=1) # For now, just drop NA's (rows with missing values)\n",
    "\n",
    "\n",
    "    # The CSV file has no column heads, so add them\n",
    "    df.columns = [\n",
    "        'time',\n",
    "        'duration',\n",
    "        'sip',\n",
    "        'dip',\n",
    "        'source_port',\n",
    "        'dest_port',\n",
    "        'protocol',\n",
    "        'flags',\n",
    "        'forward_status',\n",
    "        'type_service',\n",
    "        'pack_exch',\n",
    "        'bytes',\n",
    "        'attack_tag'\n",
    "    ]\n",
    "\n",
    "    #print(df[0:3])\n",
    "\n",
    "    ENCODING = 'utf-8'\n",
    "\n",
    "    def expand_categories(values):\n",
    "        result = []\n",
    "        s = values.value_counts()\n",
    "        t = float(len(values))\n",
    "        for v in s.index:\n",
    "            result.append(\"{}:{}%\".format(v,round(100*(s[v]/t),2)))\n",
    "        return \"[{}]\".format(\",\".join(result))\n",
    "            \n",
    "    def analyze(filename):\n",
    "        print()\n",
    "        print(\"Analyzing: {}\".format(filename))\n",
    "        df = pd.read_csv(filename,encoding=ENCODING)\n",
    "        cols = df.columns.values\n",
    "        total = float(len(df))\n",
    "\n",
    "        print(\"{} rows\".format(int(total)))\n",
    "        for col in cols:\n",
    "            uniques = df[col].unique()\n",
    "            unique_count = len(uniques)\n",
    "            if unique_count>100:\n",
    "                print(\"** {}:{} ({}%)\".format(col,unique_count,int(((unique_count)/total)*100)))\n",
    "            else:\n",
    "                print(\"** {}:{}\".format(col,expand_categories(df[col])))\n",
    "                expand_categories(df[col])\n",
    "\n",
    "    #analyze(path)\n",
    "\n",
    "    ######RESULT######\n",
    "    #####################################################\n",
    "    # 9999998 rows\n",
    "    # ** 2016-03-18 10:52:40:10719 (0%)\n",
    "    # ** 0.000:35412 (0%)\n",
    "    # ** 127.204.60.89:892565 (8%)\n",
    "    # ** 42.219.153.89:4098 (0%)\n",
    "    # ** 123:65536 (0%)\n",
    "    # ** 425:64147 (0%)\n",
    "    # ** UDP:[TCP:62.27%,UDP:36.56%,ICMP:1.05%,GRE:0.06%,ESP:0.05%,IPIP:0.01%,IPv6:0.0%]\n",
    "    # ** .A....:[.A....:40.27%,.AP.SF:26.33%,.AP.S.:7.04%,.AP...:6.6%,.APRSF:4.37%,.A...F:4.04%,....S.:2.5%,.APRS.:2.18%,.AP..F:1.85%,.A..SF:1.66%,.A.R..:0.87%,.A..S.:0.64%,.A.R.F:0.45%,...R..:0.45%,.APR..:0.44%,.APR.F:0.18%,.A.RS.:0.06%,...RS.:0.04%,.A.RSF:0.02%,......:0.01%,UAP..F:0.0%,UAP.S.:0.0%,..P.S.:0.0%]\n",
    "    # ** 0:[0:100.0%]\n",
    "    # ** 0.1:[0:66.46%,40:20.13%,72:8.62%,8:1.48%,64:1.3%,42:0.76%,24:0.4%,2:0.39%,26:0.17%,104:0.07%,75:0.05%,16:0.04%,20:0.02%,192:0.02%,74:0.02%,96:0.01%,28:0.01%,4:0.01%,224:0.01%,43:0.01%,6:0.0%,18:0.0%,12:0.0%,184:0.0%,32:0.0%,73:0.0%,10:0.0%,152:0.0%,23:0.0%,88:0.0%,13:0.0%,3:0.0%,48:0.0%,66:0.0%,160:0.0%,56:0.0%,194:0.0%,80:0.0%,15:0.0%,25:0.0%,9:0.0%,1:0.0%,17:0.0%,14:0.0%,92:0.0%,200:0.0%,19:0.0%,21:0.0%,22:0.0%,30:0.0%,41:0.0%,5:0.0%,240:0.0%,11:0.0%,27:0.0%,68:0.0%,7:0.0%,136:0.0%,29:0.0%]\n",
    "    # ** 1:8245 (0%)\n",
    "    # ** 76:116716 (1%)\n",
    "    # ** background:[background:99.7%,blacklist:0.28%,anomaly-spam:0.02%]\n",
    "    ########################################################\n",
    "\n",
    "    # Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "    def encode_text_dummy(df, name):\n",
    "        dummies = pd.get_dummies(df[name])\n",
    "        for x in dummies.columns:\n",
    "            dummy_name = f\"{name}-{x}\"\n",
    "            df[dummy_name] = dummies[x]\n",
    "        df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "    #Clean 'date' column and convert to Int type\n",
    "    def clean_date(s):\n",
    "        s = ''.join([i for i in s if i not in frozenset(string.punctuation)])\n",
    "        s_removed = s.replace(\" \", \"\")\n",
    "        s_int = int(s_removed)\n",
    "        return s_int\n",
    "\n",
    "    ########## CLEAN IP #######################\n",
    "    def clean_ip(s):\n",
    "        s = ''.join([i for i in s if i not in frozenset(string.punctuation)])\n",
    "        s_int = int(s)\n",
    "        return s_int\n",
    "\n",
    "    # Encode a numeric column as zscores\n",
    "    def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "        if mean is None:\n",
    "            mean = df[name].mean()\n",
    "\n",
    "        if sd is None:\n",
    "            sd = df[name].std()\n",
    "\n",
    "        df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "    # Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "    def encode_text_index(df, name):\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        df[name] = le.fit_transform(df[name])\n",
    "        return le.classes_\n",
    "\n",
    "    #LAS QUE YA SON NUMEROS --> no los normalizo de momento\n",
    "    # encode_numeric_zscore(df, 'duration')\n",
    "    # encode_numeric_zscore(df, 'source_port')\n",
    "    # encode_numeric_zscore(df, 'dest_port')\n",
    "    # encode_numeric_zscore(df, 'forward_status')\n",
    "    # encode_numeric_zscore(df, 'type_service')\n",
    "    # encode_numeric_zscore(df, 'pack_exch')\n",
    "    # encode_numeric_zscore(df, 'bytes')\n",
    "\n",
    "    df.drop('time', 1, inplace=True)\n",
    "\n",
    "    encode_text_dummy(df, 'protocol')\n",
    "    encode_text_dummy(df, 'flags')\n",
    "    #encode_text_dummy(df, 'attack_tag')\n",
    "\n",
    "    outcomes = encode_text_index(df, 'attack_tag')\n",
    "    num_classes = len(outcomes)\n",
    "\n",
    "    #Me crea una columna AL FINAL nueva con los valores transformdos asi 20160318105240\n",
    "    #df['time'] = df['time'].apply(clean_date)\n",
    "\n",
    "    df['sip'] = df['sip'].apply(clean_ip)\n",
    "    df['dip'] = df['dip'].apply(clean_ip)\n",
    "\n",
    "    #encode_numeric_zscore(df, 'time')\n",
    "    encode_numeric_zscore(df, 'sip')\n",
    "    encode_numeric_zscore(df, 'dip')\n",
    "\n",
    "\n",
    "    print(df.shape)\n",
    "    print(df[0:3])\n",
    "\n",
    "    # Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "    def to_xy(df, target):\n",
    "        result = []\n",
    "        for x in df.columns:\n",
    "            if x != target:\n",
    "                result.append(x)\n",
    "        # find out the type of the target column.  Is it really this hard? :(\n",
    "        target_type = df[target].dtypes\n",
    "        target_type = target_type[0] if hasattr(\n",
    "            target_type, '__iter__') else target_type\n",
    "        # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "        if target_type in (np.int64, np.int32):\n",
    "            # Classification\n",
    "            dummies = pd.get_dummies(df[target])\n",
    "            return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "        # Regression\n",
    "        return df[result].values.astype(np.float32), df[[target]].values.astype(np.float32)\n",
    "\n",
    "\n",
    "    ##################################################\n",
    "    # Break into X (predictors) & y (prediction)\n",
    "    x, y = to_xy(df,'attack_tag')\n",
    "    #################################################\n",
    "\n",
    "\n",
    "\n",
    "    # Create a test/train split.  25% test\n",
    "    # Split into train/test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "    # Create neural net\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(50, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.add(Dense(y.shape[1],activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "    # model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=1000)\n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),verbose=2,epochs=500)\n",
    "\n",
    "\n",
    "\n",
    "    # Measure accuracy\n",
    "    pred = model.predict(x_test)\n",
    "    pred = np.argmax(pred,axis=1)\n",
    "    y_eval = np.argmax(y_test,axis=1)\n",
    "    score = metrics.accuracy_score(y_eval, pred)\n",
    "    print(\"Validation score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48.38       -0.15406479 -0.62989    ...  0.          0.\n",
      "   0.        ]\n",
      " [48.38       -0.60556895 -0.18952788 ...  0.          0.\n",
      "   0.        ]\n",
      " [50.632       0.4215058   0.85083455 ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.         -0.5121985  -0.63662624 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.         -0.5121985  -0.63950783 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.         -0.5121985  -0.63656384 ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   duration       sip       dip  source_port  dest_port  forward_status  \\\n",
      "0     48.38 -0.154065 -0.629890           53         53               0   \n",
      "1     48.38 -0.605569 -0.189528           53         53               0   \n",
      "\n",
      "   type_service  pack_exch  bytes  attack_tag  ...  flags-.A.RS.  \\\n",
      "0             0          2    209           0  ...             0   \n",
      "1             0          2    167           0  ...             0   \n",
      "\n",
      "   flags-.A.RSF  flags-.AP...  flags-.AP..F  flags-.AP.S.  flags-.AP.SF  \\\n",
      "0             0             0             0             0             0   \n",
      "1             0             0             0             0             0   \n",
      "\n",
      "   flags-.APR..  flags-.APR.F  flags-.APRS.  flags-.APRSF  \n",
      "0             0             0             0             0  \n",
      "1             0             0             0             0  \n",
      "\n",
      "[2 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150000 samples, validate on 50000 samples\n",
      "Epoch 1/10\n",
      " - 6s - loss: 0.0558 - acc: 0.9960 - val_loss: 0.0601 - val_acc: 0.9963\n",
      "Epoch 2/10\n",
      " - 6s - loss: 0.0518 - acc: 0.9968 - val_loss: 0.0602 - val_acc: 0.9963\n",
      "Epoch 3/10\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 4/10\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 5/10\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 6/10\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 7/10\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 8/10\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 9/10\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 10/10\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Validation score: 0.99626\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-861e811ab6b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation score: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# Create neural net\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(50, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "    # model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=1000)\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),verbose=2,epochs=10)\n",
    "\n",
    "\n",
    "\n",
    "    # Measure accuracy\n",
    "pred = model.predict(x_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_eval = np.argmax(y_test,axis=1)\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "print(\"Validation score: {}\".format(score))\n",
    "confusion_matrix(x, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6424522c7668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(x, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150000 samples, validate on 50000 samples\n",
      "Epoch 1/10\n",
      " - 7s - loss: 0.0569 - acc: 0.9960 - val_loss: 0.0602 - val_acc: 0.9963\n",
      "Epoch 2/10\n",
      " - 7s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 3/10\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 4/10\n",
      " - 7s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 5/10\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 6/10\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 7/10\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 8/10\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 9/10\n",
      " - 7s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 10/10\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Validation score: 0.99626\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected sequence or array-like, got <class 'keras.callbacks.History'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-89c6d491c956>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation score: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/miniconda3/envs/tf_jupyter/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/tf_jupyter/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/tf_jupyter/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \"\"\"\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/tf_jupyter/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \"\"\"\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/tf_jupyter/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             raise TypeError(\"Expected sequence or array-like, got %s\" %\n\u001b[0;32m--> 142\u001b[0;31m                             type(x))\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected sequence or array-like, got <class 'keras.callbacks.History'>"
     ]
    }
   ],
   "source": [
    "# Create neural net\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(50, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "    # model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=1000)\n",
    "y_pred = model.fit(x_train,y_train,validation_data=(x_test,y_test),verbose=2,epochs=10)\n",
    "\n",
    "\n",
    "\n",
    "    # Measure accuracy\n",
    "pred = model.predict(x_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_eval = np.argmax(y_test,axis=1)\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "print(\"Validation score: {}\".format(score))\n",
    "confusion_matrix(x, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150000 samples, validate on 50000 samples\n",
      "Epoch 1/10\n",
      " - 7s - loss: 0.0527 - acc: 0.9968 - val_loss: 0.0601 - val_acc: 0.9963\n",
      "Epoch 2/10\n",
      " - 7s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0602 - val_acc: 0.9963\n",
      "Epoch 3/10\n",
      " - 7s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 4/10\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 5/10\n",
      " - 7s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 6/10\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 7/10\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 8/10\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 9/10\n",
      " - 6s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Epoch 10/10\n",
      " - 7s - loss: 0.0519 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9963\n",
      "Validation score: 0.99626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[49813,     0],\n",
       "       [  187,     0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create neural net\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(50, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "    # model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=1000)\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),verbose=2,epochs=10)\n",
    "\n",
    "\n",
    "\n",
    "    # Measure accuracy\n",
    "pred = model.predict(x_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_eval = np.argmax(y_test,axis=1)\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "print(\"Validation score: {}\".format(score))\n",
    "confusion_matrix(y_eval, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f1e26274ef73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                           \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                           \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                           cmap=plt.cm.Blues):\n\u001b[0m\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[1;32m      7\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mprints\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mplots\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mconfusion\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "class_names = num_classes\n",
    "def plot_confusion_matrix(y_eval, pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f1e26274ef73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Plot non-normalized confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m plot_confusion_matrix(y_test, y_pred, classes=class_names,\n\u001b[0;32m---> 60\u001b[0;31m                       title='Confusion matrix, without normalization')\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# Plot normalized confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-f1e26274ef73>\u001b[0m in \u001b[0;36mplot_confusion_matrix\u001b[0;34m(y_eval, pred, classes, normalize, title, cmap)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Compute confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Only use the labels that appear in the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "class_names = num_classes\n",
    "def plot_confusion_matrix(y_eval, pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = num_classes\n",
    "def plot_confusion_matrix(y_eval, pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2-Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tf_jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
