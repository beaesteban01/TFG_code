{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TextFileReader' object has no attribute 'dropna'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-303fa7cde4ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;31m#print(\"Read {} rows.\".format(len(df)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m# df = df.sample(frac=0.1, replace=False) # Uncomment this line to sample only 10% of the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# For now, just drop NA's (rows with missing values)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TextFileReader' object has no attribute 'dropna'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow.contrib.learn as skflow\n",
    "import string\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#####################################################\n",
    "#os.system('python 22_05_Transform_columns.py')\n",
    "#No puedo ponerlo asi por que entonces dice que no esta definido df\n",
    "#Seguro que hay alguna manera\n",
    "#####################################################\n",
    "\n",
    "path = \"../july_reduced copia.csv\"\n",
    "# This file is a CSV, just no CSV extension or headers\n",
    "#df_not_chunk = pd.read_csv(path, header=None, chunksize=200000)\n",
    "df = pd.read_csv(path, header=None, chunksize=200000)\n",
    "\n",
    "def expand_categories(values):\n",
    "    result = []\n",
    "    s = values.value_counts()\n",
    "    t = float(len(values))\n",
    "    for v in s.index:\n",
    "        result.append(\"{}:{}%\".format(v,round(100*(s[v]/t),2)))\n",
    "    return \"[{}]\".format(\",\".join(result))\n",
    "        \n",
    "def analyze(filename):\n",
    "    print()\n",
    "    print(\"Analyzing: {}\".format(filename))\n",
    "    df = pd.read_csv(filename,encoding=ENCODING)\n",
    "    cols = df.columns.values\n",
    "    total = float(len(df))\n",
    "\n",
    "    print(\"{} rows\".format(int(total)))\n",
    "    for col in cols:\n",
    "        uniques = df[col].unique()\n",
    "        unique_count = len(uniques)\n",
    "        if unique_count>100:\n",
    "            print(\"** {}:{} ({}%)\".format(col,unique_count,int(((unique_count)/total)*100)))\n",
    "        else:\n",
    "            print(\"** {}:{}\".format(col,expand_categories(df[col])))\n",
    "            expand_categories(df[col])\n",
    "\n",
    "#analyze(path)\n",
    "\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = f\"{name}-{x}\"\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "#Clean 'date' column and convert to Int type\n",
    "def clean_date(s):\n",
    "    s = ''.join([i for i in s if i not in frozenset(string.punctuation)])\n",
    "    s_removed = s.replace(\" \", \"\")\n",
    "    s_int = int(s_removed)\n",
    "    return s_int\n",
    "\n",
    "########## CLEAN IP #######################\n",
    "def clean_ip(s):\n",
    "    s = ''.join([i for i in s if i not in frozenset(string.punctuation)])\n",
    "    s_int = int(s)\n",
    "    return s_int\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(\n",
    "        target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    # Regression\n",
    "    return df[result].values.astype(np.float32), df[[target]].values.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "##########################CHUNKSIZE###################################################\n",
    "\n",
    "#Each chunk is in a df format\n",
    "#for df in df_not_chunk:\n",
    "    #si no imprime no funciona, da error en el print\n",
    "    #print(df[0:2])\n",
    "   # df_not_chunk = df\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "#print(\"Read {} rows.\".format(len(df)))\n",
    "# df = df.sample(frac=0.1, replace=False) # Uncomment this line to sample only 10% of the dataset\n",
    "df.dropna(inplace=True,axis=1) # For now, just drop NA's (rows with missing values)\n",
    "\n",
    "\n",
    "# The CSV file has no column heads, so add them\n",
    "df.columns = [\n",
    "    'time',\n",
    "    'duration',\n",
    "    'sip',\n",
    "    'dip',\n",
    "    'source_port',\n",
    "    'dest_port',\n",
    "    'protocol',\n",
    "    'flags',\n",
    "    'forward_status',\n",
    "    'type_service',\n",
    "    'pack_exch',\n",
    "    'bytes',\n",
    "    'attack_tag'\n",
    "]\n",
    "\n",
    "#print(df[0:3])\n",
    "\n",
    "ENCODING = 'utf-8'\n",
    "\n",
    "\n",
    "\n",
    "#LAS QUE YA SON NUMEROS --> no los normalizo de momento\n",
    "# encode_numeric_zscore(df, 'duration')\n",
    "# encode_numeric_zscore(df, 'source_port')\n",
    "# encode_numeric_zscore(df, 'dest_port')\n",
    "# encode_numeric_zscore(df, 'forward_status')\n",
    "# encode_numeric_zscore(df, 'type_service')\n",
    "# encode_numeric_zscore(df, 'pack_exch')\n",
    "# encode_numeric_zscore(df, 'bytes')\n",
    "\n",
    "df.drop('time', 1, inplace=True)\n",
    "\n",
    "encode_text_dummy(df, 'protocol')\n",
    "encode_text_dummy(df, 'flags')\n",
    "#encode_text_dummy(df, 'attack_tag')\n",
    "\n",
    "outcomes = encode_text_index(df, 'attack_tag')\n",
    "num_classes = len(outcomes)\n",
    "\n",
    "#Me crea una columna AL FINAL nueva con los valores transformdos asi 20160318105240\n",
    "#df['time'] = df['time'].apply(clean_date)\n",
    "\n",
    "df['sip'] = df['sip'].apply(clean_ip)\n",
    "df['dip'] = df['dip'].apply(clean_ip)\n",
    "\n",
    "#encode_numeric_zscore(df, 'time')\n",
    "encode_numeric_zscore(df, 'sip')\n",
    "encode_numeric_zscore(df, 'dip')\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "print(df[0:3])\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Break into X (predictors) & y (prediction)\n",
    "x, y = to_xy(df,'attack_tag')\n",
    "#################################################\n",
    "\n",
    "\n",
    "\n",
    "# Create a test/train split.  25% test\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create neural net\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(50, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=5, verbose=1, mode='auto')\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=1000)\n",
    "#model.fit(x_train,y_train,validation_data=(x_test,y_test),verbose=2,epochs=500)\n",
    "\n",
    "\n",
    "\n",
    "# Measure accuracy\n",
    "pred = model.predict(x_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_eval = np.argmax(y_test,axis=1)\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "print(\"Validation score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TextFileReader' object has no attribute 'dropna'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-63aeb183f952>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m#print(\"Read {} rows.\".format(len(df)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;31m# df = df.sample(frac=0.1, replace=False) # Uncomment this line to sample only 10% of the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# For now, just drop NA's (rows with missing values)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TextFileReader' object has no attribute 'dropna'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow.contrib.learn as skflow\n",
    "import string\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#####################################################\n",
    "#os.system('python 22_05_Transform_columns.py')\n",
    "#No puedo ponerlo asi por que entonces dice que no esta definido df\n",
    "#Seguro que hay alguna manera\n",
    "#####################################################\n",
    "\n",
    "path = \"../july_reduced copia.csv\"\n",
    "# This file is a CSV, just no CSV extension or headers\n",
    "#df_not_chunk = pd.read_csv(path, header=None, chunksize=200000)\n",
    "df = pd.read_csv(path, header=None, chunksize=200000)\n",
    "\n",
    "def expand_categories(values):\n",
    "    result = []\n",
    "    s = values.value_counts()\n",
    "    t = float(len(values))\n",
    "    for v in s.index:\n",
    "        result.append(\"{}:{}%\".format(v,round(100*(s[v]/t),2)))\n",
    "    return \"[{}]\".format(\",\".join(result))\n",
    "        \n",
    "def analyze(filename):\n",
    "    print()\n",
    "    print(\"Analyzing: {}\".format(filename))\n",
    "    df = pd.read_csv(filename,encoding=ENCODING)\n",
    "    cols = df.columns.values\n",
    "    total = float(len(df))\n",
    "\n",
    "    print(\"{} rows\".format(int(total)))\n",
    "    for col in cols:\n",
    "        uniques = df[col].unique()\n",
    "        unique_count = len(uniques)\n",
    "        if unique_count>100:\n",
    "            print(\"** {}:{} ({}%)\".format(col,unique_count,int(((unique_count)/total)*100)))\n",
    "        else:\n",
    "            print(\"** {}:{}\".format(col,expand_categories(df[col])))\n",
    "            expand_categories(df[col])\n",
    "\n",
    "#analyze(path)\n",
    "\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = f\"{name}-{x}\"\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "#Clean 'date' column and convert to Int type\n",
    "def clean_date(s):\n",
    "    s = ''.join([i for i in s if i not in frozenset(string.punctuation)])\n",
    "    s_removed = s.replace(\" \", \"\")\n",
    "    s_int = int(s_removed)\n",
    "    return s_int\n",
    "\n",
    "########## CLEAN IP #######################\n",
    "def clean_ip(s):\n",
    "    s = ''.join([i for i in s if i not in frozenset(string.punctuation)])\n",
    "    s_int = int(s)\n",
    "    return s_int\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(\n",
    "        target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    # Regression\n",
    "    return df[result].values.astype(np.float32), df[[target]].values.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "##########################CHUNKSIZE###################################################\n",
    "\n",
    "#Each chunk is in a df format\n",
    "#for df in df_not_chunk:\n",
    "    #si no imprime no funciona, da error en el print\n",
    "    #print(df[0:2])\n",
    "   # df_not_chunk = df\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "#print(\"Read {} rows.\".format(len(df)))\n",
    "# df = df.sample(frac=0.1, replace=False) # Uncomment this line to sample only 10% of the dataset\n",
    "df.dropna(inplace=True,axis=1) # For now, just drop NA's (rows with missing values)\n",
    "\n",
    "\n",
    "# The CSV file has no column heads, so add them\n",
    "df.columns = [\n",
    "    'time',\n",
    "    'duration',\n",
    "    'sip',\n",
    "    'dip',\n",
    "    'source_port',\n",
    "    'dest_port',\n",
    "    'protocol',\n",
    "    'flags',\n",
    "    'forward_status',\n",
    "    'type_service',\n",
    "    'pack_exch',\n",
    "    'bytes',\n",
    "    'attack_tag'\n",
    "]\n",
    "\n",
    "#print(df[0:3])\n",
    "\n",
    "ENCODING = 'utf-8'\n",
    "\n",
    "\n",
    "\n",
    "#LAS QUE YA SON NUMEROS --> no los normalizo de momento\n",
    "# encode_numeric_zscore(df, 'duration')\n",
    "# encode_numeric_zscore(df, 'source_port')\n",
    "# encode_numeric_zscore(df, 'dest_port')\n",
    "# encode_numeric_zscore(df, 'forward_status')\n",
    "# encode_numeric_zscore(df, 'type_service')\n",
    "# encode_numeric_zscore(df, 'pack_exch')\n",
    "# encode_numeric_zscore(df, 'bytes')\n",
    "\n",
    "df.drop('time', 1, inplace=True)\n",
    "\n",
    "encode_text_dummy(df, 'protocol')\n",
    "encode_text_dummy(df, 'flags')\n",
    "#encode_text_dummy(df, 'attack_tag')\n",
    "\n",
    "outcomes = encode_text_index(df, 'attack_tag')\n",
    "num_classes = len(outcomes)\n",
    "\n",
    "#Me crea una columna AL FINAL nueva con los valores transformdos asi 20160318105240\n",
    "#df['time'] = df['time'].apply(clean_date)\n",
    "\n",
    "df['sip'] = df['sip'].apply(clean_ip)\n",
    "df['dip'] = df['dip'].apply(clean_ip)\n",
    "\n",
    "#encode_numeric_zscore(df, 'time')\n",
    "encode_numeric_zscore(df, 'sip')\n",
    "encode_numeric_zscore(df, 'dip')\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "print(df[0:3])\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Break into X (predictors) & y (prediction)\n",
    "x, y = to_xy(df,'attack_tag')\n",
    "#################################################\n",
    "\n",
    "\n",
    "\n",
    "# Create a test/train split.  25% test\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create neural net\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(50, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=5, verbose=1, mode='auto')\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=1000)\n",
    "#model.fit(x_train,y_train,validation_data=(x_test,y_test),verbose=2,epochs=500)\n",
    "\n",
    "\n",
    "\n",
    "# Measure accuracy\n",
    "pred = model.predict(x_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_eval = np.argmax(y_test,axis=1)\n",
    "score = metrics.accuracy_score(y_eval, pred)\n",
    "print(\"Validation score: {}\".format(score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2-Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tf_jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
